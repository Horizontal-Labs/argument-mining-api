{
  "best_global_step": 4911,
  "best_metric": 0.5653634667396545,
  "best_model_checkpoint": "./argument-mining-modernbert-stance_classification\\checkpoint-4911",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 4911,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.012217470983506415,
      "grad_norm": 2.0373775959014893,
      "learning_rate": 7.723577235772359e-07,
      "loss": 0.7287,
      "step": 20
    },
    {
      "epoch": 0.02443494196701283,
      "grad_norm": 6.21848201751709,
      "learning_rate": 1.5853658536585368e-06,
      "loss": 0.7543,
      "step": 40
    },
    {
      "epoch": 0.03665241295051924,
      "grad_norm": 1.811694622039795,
      "learning_rate": 2.3983739837398375e-06,
      "loss": 0.7143,
      "step": 60
    },
    {
      "epoch": 0.04886988393402566,
      "grad_norm": 2.6934807300567627,
      "learning_rate": 3.211382113821139e-06,
      "loss": 0.7171,
      "step": 80
    },
    {
      "epoch": 0.06108735491753207,
      "grad_norm": 2.476107358932495,
      "learning_rate": 4.024390243902439e-06,
      "loss": 0.7551,
      "step": 100
    },
    {
      "epoch": 0.07330482590103848,
      "grad_norm": 2.7702202796936035,
      "learning_rate": 4.83739837398374e-06,
      "loss": 0.7086,
      "step": 120
    },
    {
      "epoch": 0.0855222968845449,
      "grad_norm": 2.1743104457855225,
      "learning_rate": 5.650406504065041e-06,
      "loss": 0.7222,
      "step": 140
    },
    {
      "epoch": 0.09773976786805132,
      "grad_norm": 4.131826877593994,
      "learning_rate": 6.463414634146342e-06,
      "loss": 0.6897,
      "step": 160
    },
    {
      "epoch": 0.10995723885155773,
      "grad_norm": 1.7771012783050537,
      "learning_rate": 7.276422764227643e-06,
      "loss": 0.7024,
      "step": 180
    },
    {
      "epoch": 0.12217470983506414,
      "grad_norm": 1.930648684501648,
      "learning_rate": 8.089430894308944e-06,
      "loss": 0.7283,
      "step": 200
    },
    {
      "epoch": 0.13439218081857054,
      "grad_norm": 2.3677263259887695,
      "learning_rate": 8.902439024390244e-06,
      "loss": 0.7012,
      "step": 220
    },
    {
      "epoch": 0.14660965180207697,
      "grad_norm": 1.4683295488357544,
      "learning_rate": 9.715447154471546e-06,
      "loss": 0.7078,
      "step": 240
    },
    {
      "epoch": 0.1588271227855834,
      "grad_norm": 2.5203583240509033,
      "learning_rate": 1.0528455284552846e-05,
      "loss": 0.7181,
      "step": 260
    },
    {
      "epoch": 0.1710445937690898,
      "grad_norm": 2.4517641067504883,
      "learning_rate": 1.1341463414634146e-05,
      "loss": 0.7155,
      "step": 280
    },
    {
      "epoch": 0.1832620647525962,
      "grad_norm": 2.7344753742218018,
      "learning_rate": 1.2154471544715448e-05,
      "loss": 0.7079,
      "step": 300
    },
    {
      "epoch": 0.19547953573610263,
      "grad_norm": 2.1329305171966553,
      "learning_rate": 1.2967479674796748e-05,
      "loss": 0.7072,
      "step": 320
    },
    {
      "epoch": 0.20769700671960903,
      "grad_norm": 2.6435976028442383,
      "learning_rate": 1.378048780487805e-05,
      "loss": 0.6989,
      "step": 340
    },
    {
      "epoch": 0.21991447770311545,
      "grad_norm": 3.4428904056549072,
      "learning_rate": 1.459349593495935e-05,
      "loss": 0.719,
      "step": 360
    },
    {
      "epoch": 0.23213194868662188,
      "grad_norm": 3.0865113735198975,
      "learning_rate": 1.540650406504065e-05,
      "loss": 0.7078,
      "step": 380
    },
    {
      "epoch": 0.24434941967012827,
      "grad_norm": 2.5478451251983643,
      "learning_rate": 1.6219512195121953e-05,
      "loss": 0.7039,
      "step": 400
    },
    {
      "epoch": 0.2565668906536347,
      "grad_norm": 2.1825785636901855,
      "learning_rate": 1.7032520325203255e-05,
      "loss": 0.7005,
      "step": 420
    },
    {
      "epoch": 0.2687843616371411,
      "grad_norm": 3.114396333694458,
      "learning_rate": 1.7845528455284557e-05,
      "loss": 0.7111,
      "step": 440
    },
    {
      "epoch": 0.28100183262064754,
      "grad_norm": 2.344566822052002,
      "learning_rate": 1.8658536585365855e-05,
      "loss": 0.7172,
      "step": 460
    },
    {
      "epoch": 0.29321930360415394,
      "grad_norm": 4.31289529800415,
      "learning_rate": 1.9471544715447157e-05,
      "loss": 0.722,
      "step": 480
    },
    {
      "epoch": 0.30543677458766033,
      "grad_norm": 2.6467597484588623,
      "learning_rate": 1.9968318624123107e-05,
      "loss": 0.7071,
      "step": 500
    },
    {
      "epoch": 0.3176542455711668,
      "grad_norm": 4.7676005363464355,
      "learning_rate": 1.9877800407331976e-05,
      "loss": 0.6983,
      "step": 520
    },
    {
      "epoch": 0.3298717165546732,
      "grad_norm": 2.3608837127685547,
      "learning_rate": 1.9787282190540848e-05,
      "loss": 0.7094,
      "step": 540
    },
    {
      "epoch": 0.3420891875381796,
      "grad_norm": 3.9424924850463867,
      "learning_rate": 1.969676397374972e-05,
      "loss": 0.7024,
      "step": 560
    },
    {
      "epoch": 0.354306658521686,
      "grad_norm": 3.015151023864746,
      "learning_rate": 1.9606245756958588e-05,
      "loss": 0.6888,
      "step": 580
    },
    {
      "epoch": 0.3665241295051924,
      "grad_norm": 4.9558281898498535,
      "learning_rate": 1.951572754016746e-05,
      "loss": 0.6904,
      "step": 600
    },
    {
      "epoch": 0.3787416004886988,
      "grad_norm": 4.024746894836426,
      "learning_rate": 1.942520932337633e-05,
      "loss": 0.6881,
      "step": 620
    },
    {
      "epoch": 0.39095907147220527,
      "grad_norm": 2.83225679397583,
      "learning_rate": 1.93346911065852e-05,
      "loss": 0.6992,
      "step": 640
    },
    {
      "epoch": 0.40317654245571166,
      "grad_norm": 5.4416399002075195,
      "learning_rate": 1.9244172889794072e-05,
      "loss": 0.7051,
      "step": 660
    },
    {
      "epoch": 0.41539401343921806,
      "grad_norm": 1.5405933856964111,
      "learning_rate": 1.9153654673002944e-05,
      "loss": 0.7006,
      "step": 680
    },
    {
      "epoch": 0.4276114844227245,
      "grad_norm": 5.420446872711182,
      "learning_rate": 1.9063136456211816e-05,
      "loss": 0.7125,
      "step": 700
    },
    {
      "epoch": 0.4398289554062309,
      "grad_norm": 1.4525525569915771,
      "learning_rate": 1.8972618239420687e-05,
      "loss": 0.6975,
      "step": 720
    },
    {
      "epoch": 0.4520464263897373,
      "grad_norm": 4.470304012298584,
      "learning_rate": 1.8882100022629556e-05,
      "loss": 0.6888,
      "step": 740
    },
    {
      "epoch": 0.46426389737324375,
      "grad_norm": 1.5053820610046387,
      "learning_rate": 1.8791581805838424e-05,
      "loss": 0.6857,
      "step": 760
    },
    {
      "epoch": 0.47648136835675015,
      "grad_norm": 2.456831455230713,
      "learning_rate": 1.8701063589047296e-05,
      "loss": 0.6984,
      "step": 780
    },
    {
      "epoch": 0.48869883934025654,
      "grad_norm": 3.190539598464966,
      "learning_rate": 1.8610545372256168e-05,
      "loss": 0.7028,
      "step": 800
    },
    {
      "epoch": 0.5009163103237629,
      "grad_norm": 2.590606451034546,
      "learning_rate": 1.852002715546504e-05,
      "loss": 0.6894,
      "step": 820
    },
    {
      "epoch": 0.5131337813072694,
      "grad_norm": 1.786249041557312,
      "learning_rate": 1.842950893867391e-05,
      "loss": 0.6855,
      "step": 840
    },
    {
      "epoch": 0.5253512522907758,
      "grad_norm": 2.7720580101013184,
      "learning_rate": 1.833899072188278e-05,
      "loss": 0.6831,
      "step": 860
    },
    {
      "epoch": 0.5375687232742822,
      "grad_norm": 1.4907150268554688,
      "learning_rate": 1.8248472505091652e-05,
      "loss": 0.6831,
      "step": 880
    },
    {
      "epoch": 0.5497861942577886,
      "grad_norm": 3.0204944610595703,
      "learning_rate": 1.815795428830052e-05,
      "loss": 0.6918,
      "step": 900
    },
    {
      "epoch": 0.5620036652412951,
      "grad_norm": 2.7649240493774414,
      "learning_rate": 1.8067436071509392e-05,
      "loss": 0.6817,
      "step": 920
    },
    {
      "epoch": 0.5742211362248014,
      "grad_norm": 1.2296977043151855,
      "learning_rate": 1.7976917854718264e-05,
      "loss": 0.6828,
      "step": 940
    },
    {
      "epoch": 0.5864386072083079,
      "grad_norm": 2.676525592803955,
      "learning_rate": 1.7886399637927136e-05,
      "loss": 0.6937,
      "step": 960
    },
    {
      "epoch": 0.5986560781918143,
      "grad_norm": 1.9281580448150635,
      "learning_rate": 1.7795881421136004e-05,
      "loss": 0.6806,
      "step": 980
    },
    {
      "epoch": 0.6108735491753207,
      "grad_norm": 2.4988036155700684,
      "learning_rate": 1.7705363204344876e-05,
      "loss": 0.6782,
      "step": 1000
    },
    {
      "epoch": 0.6230910201588271,
      "grad_norm": 1.460666298866272,
      "learning_rate": 1.7614844987553748e-05,
      "loss": 0.7022,
      "step": 1020
    },
    {
      "epoch": 0.6353084911423336,
      "grad_norm": 3.5542030334472656,
      "learning_rate": 1.7524326770762616e-05,
      "loss": 0.7007,
      "step": 1040
    },
    {
      "epoch": 0.6475259621258399,
      "grad_norm": 3.6573739051818848,
      "learning_rate": 1.7433808553971488e-05,
      "loss": 0.7043,
      "step": 1060
    },
    {
      "epoch": 0.6597434331093464,
      "grad_norm": 2.220463514328003,
      "learning_rate": 1.734329033718036e-05,
      "loss": 0.6887,
      "step": 1080
    },
    {
      "epoch": 0.6719609040928528,
      "grad_norm": 1.5749709606170654,
      "learning_rate": 1.725277212038923e-05,
      "loss": 0.6791,
      "step": 1100
    },
    {
      "epoch": 0.6841783750763591,
      "grad_norm": 3.500483751296997,
      "learning_rate": 1.71622539035981e-05,
      "loss": 0.6797,
      "step": 1120
    },
    {
      "epoch": 0.6963958460598656,
      "grad_norm": 2.623767852783203,
      "learning_rate": 1.7071735686806972e-05,
      "loss": 0.6674,
      "step": 1140
    },
    {
      "epoch": 0.708613317043372,
      "grad_norm": 2.1475675106048584,
      "learning_rate": 1.6981217470015844e-05,
      "loss": 0.699,
      "step": 1160
    },
    {
      "epoch": 0.7208307880268784,
      "grad_norm": 1.8889023065567017,
      "learning_rate": 1.6890699253224712e-05,
      "loss": 0.6907,
      "step": 1180
    },
    {
      "epoch": 0.7330482590103848,
      "grad_norm": 5.5560688972473145,
      "learning_rate": 1.6800181036433584e-05,
      "loss": 0.6924,
      "step": 1200
    },
    {
      "epoch": 0.7452657299938913,
      "grad_norm": 2.6856112480163574,
      "learning_rate": 1.6709662819642453e-05,
      "loss": 0.6921,
      "step": 1220
    },
    {
      "epoch": 0.7574832009773976,
      "grad_norm": 1.420727014541626,
      "learning_rate": 1.6619144602851324e-05,
      "loss": 0.6954,
      "step": 1240
    },
    {
      "epoch": 0.7697006719609041,
      "grad_norm": 1.6514866352081299,
      "learning_rate": 1.6528626386060196e-05,
      "loss": 0.6861,
      "step": 1260
    },
    {
      "epoch": 0.7819181429444105,
      "grad_norm": 3.1688284873962402,
      "learning_rate": 1.6438108169269068e-05,
      "loss": 0.6911,
      "step": 1280
    },
    {
      "epoch": 0.7941356139279169,
      "grad_norm": 4.937164306640625,
      "learning_rate": 1.6347589952477937e-05,
      "loss": 0.6979,
      "step": 1300
    },
    {
      "epoch": 0.8063530849114233,
      "grad_norm": 1.644557237625122,
      "learning_rate": 1.625707173568681e-05,
      "loss": 0.6946,
      "step": 1320
    },
    {
      "epoch": 0.8185705558949298,
      "grad_norm": 3.8674111366271973,
      "learning_rate": 1.616655351889568e-05,
      "loss": 0.6786,
      "step": 1340
    },
    {
      "epoch": 0.8307880268784361,
      "grad_norm": 3.6418633460998535,
      "learning_rate": 1.607603530210455e-05,
      "loss": 0.6723,
      "step": 1360
    },
    {
      "epoch": 0.8430054978619426,
      "grad_norm": 1.6722573041915894,
      "learning_rate": 1.598551708531342e-05,
      "loss": 0.6903,
      "step": 1380
    },
    {
      "epoch": 0.855222968845449,
      "grad_norm": 3.2545723915100098,
      "learning_rate": 1.5894998868522292e-05,
      "loss": 0.6903,
      "step": 1400
    },
    {
      "epoch": 0.8674404398289554,
      "grad_norm": 3.3494324684143066,
      "learning_rate": 1.580448065173116e-05,
      "loss": 0.6895,
      "step": 1420
    },
    {
      "epoch": 0.8796579108124618,
      "grad_norm": 6.229945659637451,
      "learning_rate": 1.5713962434940033e-05,
      "loss": 0.6882,
      "step": 1440
    },
    {
      "epoch": 0.8918753817959683,
      "grad_norm": 1.1468675136566162,
      "learning_rate": 1.5623444218148904e-05,
      "loss": 0.6978,
      "step": 1460
    },
    {
      "epoch": 0.9040928527794746,
      "grad_norm": 1.7545254230499268,
      "learning_rate": 1.5532926001357776e-05,
      "loss": 0.6886,
      "step": 1480
    },
    {
      "epoch": 0.916310323762981,
      "grad_norm": 2.0930733680725098,
      "learning_rate": 1.5442407784566645e-05,
      "loss": 0.69,
      "step": 1500
    },
    {
      "epoch": 0.9285277947464875,
      "grad_norm": 3.5744125843048096,
      "learning_rate": 1.5351889567775517e-05,
      "loss": 0.6925,
      "step": 1520
    },
    {
      "epoch": 0.9407452657299938,
      "grad_norm": 1.7362152338027954,
      "learning_rate": 1.5261371350984385e-05,
      "loss": 0.6751,
      "step": 1540
    },
    {
      "epoch": 0.9529627367135003,
      "grad_norm": 2.6113972663879395,
      "learning_rate": 1.5170853134193257e-05,
      "loss": 0.6959,
      "step": 1560
    },
    {
      "epoch": 0.9651802076970067,
      "grad_norm": 2.3913097381591797,
      "learning_rate": 1.5080334917402129e-05,
      "loss": 0.6929,
      "step": 1580
    },
    {
      "epoch": 0.9773976786805131,
      "grad_norm": 1.2919442653656006,
      "learning_rate": 1.4989816700610999e-05,
      "loss": 0.6854,
      "step": 1600
    },
    {
      "epoch": 0.9896151496640195,
      "grad_norm": 5.008941173553467,
      "learning_rate": 1.489929848381987e-05,
      "loss": 0.6908,
      "step": 1620
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.5552840828895569,
      "eval_loss": 0.6857232451438904,
      "eval_runtime": 111.841,
      "eval_samples_per_second": 58.547,
      "eval_steps_per_second": 7.323,
      "step": 1637
    },
    {
      "epoch": 1.0018326206475259,
      "grad_norm": 4.256256103515625,
      "learning_rate": 1.4808780267028742e-05,
      "loss": 0.681,
      "step": 1640
    },
    {
      "epoch": 1.0140500916310324,
      "grad_norm": 5.143985748291016,
      "learning_rate": 1.4718262050237611e-05,
      "loss": 0.7053,
      "step": 1660
    },
    {
      "epoch": 1.0262675626145388,
      "grad_norm": 2.8409178256988525,
      "learning_rate": 1.4627743833446481e-05,
      "loss": 0.6911,
      "step": 1680
    },
    {
      "epoch": 1.0384850335980451,
      "grad_norm": 3.4763259887695312,
      "learning_rate": 1.4537225616655353e-05,
      "loss": 0.678,
      "step": 1700
    },
    {
      "epoch": 1.0507025045815517,
      "grad_norm": 2.481757640838623,
      "learning_rate": 1.4446707399864223e-05,
      "loss": 0.6907,
      "step": 1720
    },
    {
      "epoch": 1.062919975565058,
      "grad_norm": 1.6640840768814087,
      "learning_rate": 1.4356189183073095e-05,
      "loss": 0.6724,
      "step": 1740
    },
    {
      "epoch": 1.0751374465485644,
      "grad_norm": 3.5133726596832275,
      "learning_rate": 1.4265670966281967e-05,
      "loss": 0.6687,
      "step": 1760
    },
    {
      "epoch": 1.087354917532071,
      "grad_norm": 2.6920034885406494,
      "learning_rate": 1.4175152749490837e-05,
      "loss": 0.6873,
      "step": 1780
    },
    {
      "epoch": 1.0995723885155773,
      "grad_norm": 1.67105233669281,
      "learning_rate": 1.4084634532699709e-05,
      "loss": 0.7074,
      "step": 1800
    },
    {
      "epoch": 1.1117898594990836,
      "grad_norm": 3.065861940383911,
      "learning_rate": 1.3994116315908577e-05,
      "loss": 0.6868,
      "step": 1820
    },
    {
      "epoch": 1.1240073304825902,
      "grad_norm": 1.884631633758545,
      "learning_rate": 1.3903598099117447e-05,
      "loss": 0.6889,
      "step": 1840
    },
    {
      "epoch": 1.1362248014660965,
      "grad_norm": 1.3071836233139038,
      "learning_rate": 1.3813079882326319e-05,
      "loss": 0.6706,
      "step": 1860
    },
    {
      "epoch": 1.1484422724496028,
      "grad_norm": 1.9332349300384521,
      "learning_rate": 1.3722561665535191e-05,
      "loss": 0.6705,
      "step": 1880
    },
    {
      "epoch": 1.1606597434331094,
      "grad_norm": 3.6097114086151123,
      "learning_rate": 1.3632043448744061e-05,
      "loss": 0.6822,
      "step": 1900
    },
    {
      "epoch": 1.1728772144166157,
      "grad_norm": 5.10127067565918,
      "learning_rate": 1.3541525231952933e-05,
      "loss": 0.7037,
      "step": 1920
    },
    {
      "epoch": 1.185094685400122,
      "grad_norm": 1.8940935134887695,
      "learning_rate": 1.3451007015161803e-05,
      "loss": 0.6679,
      "step": 1940
    },
    {
      "epoch": 1.1973121563836286,
      "grad_norm": 3.7601871490478516,
      "learning_rate": 1.3360488798370671e-05,
      "loss": 0.6832,
      "step": 1960
    },
    {
      "epoch": 1.209529627367135,
      "grad_norm": 1.2857682704925537,
      "learning_rate": 1.3269970581579543e-05,
      "loss": 0.6793,
      "step": 1980
    },
    {
      "epoch": 1.2217470983506413,
      "grad_norm": 1.2735555171966553,
      "learning_rate": 1.3179452364788415e-05,
      "loss": 0.6903,
      "step": 2000
    },
    {
      "epoch": 1.2339645693341479,
      "grad_norm": 2.4040565490722656,
      "learning_rate": 1.3088934147997285e-05,
      "loss": 0.6752,
      "step": 2020
    },
    {
      "epoch": 1.2461820403176542,
      "grad_norm": 2.2833735942840576,
      "learning_rate": 1.2998415931206157e-05,
      "loss": 0.6731,
      "step": 2040
    },
    {
      "epoch": 1.2583995113011608,
      "grad_norm": 2.333909749984741,
      "learning_rate": 1.2907897714415027e-05,
      "loss": 0.6814,
      "step": 2060
    },
    {
      "epoch": 1.2706169822846671,
      "grad_norm": 4.562553405761719,
      "learning_rate": 1.2817379497623899e-05,
      "loss": 0.6733,
      "step": 2080
    },
    {
      "epoch": 1.2828344532681735,
      "grad_norm": 2.1240787506103516,
      "learning_rate": 1.2726861280832769e-05,
      "loss": 0.6686,
      "step": 2100
    },
    {
      "epoch": 1.29505192425168,
      "grad_norm": 10.149491310119629,
      "learning_rate": 1.263634306404164e-05,
      "loss": 0.6744,
      "step": 2120
    },
    {
      "epoch": 1.3072693952351864,
      "grad_norm": 5.18981409072876,
      "learning_rate": 1.254582484725051e-05,
      "loss": 0.6889,
      "step": 2140
    },
    {
      "epoch": 1.3194868662186927,
      "grad_norm": 2.6550450325012207,
      "learning_rate": 1.2455306630459381e-05,
      "loss": 0.6799,
      "step": 2160
    },
    {
      "epoch": 1.3317043372021993,
      "grad_norm": 1.2354334592819214,
      "learning_rate": 1.2364788413668251e-05,
      "loss": 0.681,
      "step": 2180
    },
    {
      "epoch": 1.3439218081857056,
      "grad_norm": 1.7146763801574707,
      "learning_rate": 1.2274270196877123e-05,
      "loss": 0.6949,
      "step": 2200
    },
    {
      "epoch": 1.356139279169212,
      "grad_norm": 4.549584865570068,
      "learning_rate": 1.2183751980085993e-05,
      "loss": 0.6963,
      "step": 2220
    },
    {
      "epoch": 1.3683567501527185,
      "grad_norm": 2.056379556655884,
      "learning_rate": 1.2093233763294865e-05,
      "loss": 0.6719,
      "step": 2240
    },
    {
      "epoch": 1.3805742211362249,
      "grad_norm": 3.711883306503296,
      "learning_rate": 1.2002715546503735e-05,
      "loss": 0.6827,
      "step": 2260
    },
    {
      "epoch": 1.3927916921197312,
      "grad_norm": 3.8091182708740234,
      "learning_rate": 1.1912197329712605e-05,
      "loss": 0.6905,
      "step": 2280
    },
    {
      "epoch": 1.4050091631032378,
      "grad_norm": 1.869385838508606,
      "learning_rate": 1.1821679112921476e-05,
      "loss": 0.6802,
      "step": 2300
    },
    {
      "epoch": 1.417226634086744,
      "grad_norm": 2.047513723373413,
      "learning_rate": 1.1731160896130347e-05,
      "loss": 0.6668,
      "step": 2320
    },
    {
      "epoch": 1.4294441050702504,
      "grad_norm": 4.690930366516113,
      "learning_rate": 1.1640642679339218e-05,
      "loss": 0.6769,
      "step": 2340
    },
    {
      "epoch": 1.441661576053757,
      "grad_norm": 1.9165847301483154,
      "learning_rate": 1.155012446254809e-05,
      "loss": 0.7011,
      "step": 2360
    },
    {
      "epoch": 1.4538790470372633,
      "grad_norm": 3.7907707691192627,
      "learning_rate": 1.145960624575696e-05,
      "loss": 0.6926,
      "step": 2380
    },
    {
      "epoch": 1.4660965180207697,
      "grad_norm": 1.4104433059692383,
      "learning_rate": 1.1369088028965831e-05,
      "loss": 0.6947,
      "step": 2400
    },
    {
      "epoch": 1.4783139890042762,
      "grad_norm": 1.5096070766448975,
      "learning_rate": 1.12785698121747e-05,
      "loss": 0.6854,
      "step": 2420
    },
    {
      "epoch": 1.4905314599877826,
      "grad_norm": 4.455057621002197,
      "learning_rate": 1.1188051595383572e-05,
      "loss": 0.6835,
      "step": 2440
    },
    {
      "epoch": 1.502748930971289,
      "grad_norm": 1.6371036767959595,
      "learning_rate": 1.1097533378592442e-05,
      "loss": 0.6986,
      "step": 2460
    },
    {
      "epoch": 1.5149664019547955,
      "grad_norm": 3.699476957321167,
      "learning_rate": 1.1007015161801314e-05,
      "loss": 0.6803,
      "step": 2480
    },
    {
      "epoch": 1.5271838729383018,
      "grad_norm": 2.278790235519409,
      "learning_rate": 1.0916496945010184e-05,
      "loss": 0.6804,
      "step": 2500
    },
    {
      "epoch": 1.5394013439218082,
      "grad_norm": 1.5050568580627441,
      "learning_rate": 1.0825978728219056e-05,
      "loss": 0.6816,
      "step": 2520
    },
    {
      "epoch": 1.5516188149053147,
      "grad_norm": 4.544501304626465,
      "learning_rate": 1.0735460511427926e-05,
      "loss": 0.681,
      "step": 2540
    },
    {
      "epoch": 1.563836285888821,
      "grad_norm": 5.884803295135498,
      "learning_rate": 1.0644942294636798e-05,
      "loss": 0.6979,
      "step": 2560
    },
    {
      "epoch": 1.5760537568723274,
      "grad_norm": 4.221576690673828,
      "learning_rate": 1.0554424077845666e-05,
      "loss": 0.68,
      "step": 2580
    },
    {
      "epoch": 1.588271227855834,
      "grad_norm": 2.7515764236450195,
      "learning_rate": 1.0463905861054538e-05,
      "loss": 0.6806,
      "step": 2600
    },
    {
      "epoch": 1.6004886988393403,
      "grad_norm": 2.5451197624206543,
      "learning_rate": 1.0373387644263408e-05,
      "loss": 0.6863,
      "step": 2620
    },
    {
      "epoch": 1.6127061698228466,
      "grad_norm": 2.945524215698242,
      "learning_rate": 1.028286942747228e-05,
      "loss": 0.6807,
      "step": 2640
    },
    {
      "epoch": 1.6249236408063532,
      "grad_norm": 1.3909926414489746,
      "learning_rate": 1.019235121068115e-05,
      "loss": 0.6654,
      "step": 2660
    },
    {
      "epoch": 1.6371411117898595,
      "grad_norm": 1.9132845401763916,
      "learning_rate": 1.0101832993890022e-05,
      "loss": 0.6874,
      "step": 2680
    },
    {
      "epoch": 1.6493585827733659,
      "grad_norm": 3.01096248626709,
      "learning_rate": 1.0011314777098894e-05,
      "loss": 0.6786,
      "step": 2700
    },
    {
      "epoch": 1.6615760537568725,
      "grad_norm": 2.334277868270874,
      "learning_rate": 9.920796560307762e-06,
      "loss": 0.6768,
      "step": 2720
    },
    {
      "epoch": 1.6737935247403788,
      "grad_norm": 2.504852533340454,
      "learning_rate": 9.830278343516634e-06,
      "loss": 0.6978,
      "step": 2740
    },
    {
      "epoch": 1.6860109957238851,
      "grad_norm": 2.0123953819274902,
      "learning_rate": 9.739760126725504e-06,
      "loss": 0.6748,
      "step": 2760
    },
    {
      "epoch": 1.6982284667073917,
      "grad_norm": 2.0829010009765625,
      "learning_rate": 9.649241909934374e-06,
      "loss": 0.6831,
      "step": 2780
    },
    {
      "epoch": 1.710445937690898,
      "grad_norm": 2.374939203262329,
      "learning_rate": 9.558723693143246e-06,
      "loss": 0.6559,
      "step": 2800
    },
    {
      "epoch": 1.7226634086744044,
      "grad_norm": 3.585264205932617,
      "learning_rate": 9.468205476352118e-06,
      "loss": 0.6726,
      "step": 2820
    },
    {
      "epoch": 1.734880879657911,
      "grad_norm": 1.6589577198028564,
      "learning_rate": 9.377687259560986e-06,
      "loss": 0.6885,
      "step": 2840
    },
    {
      "epoch": 1.7470983506414173,
      "grad_norm": 2.913419723510742,
      "learning_rate": 9.287169042769858e-06,
      "loss": 0.6851,
      "step": 2860
    },
    {
      "epoch": 1.7593158216249236,
      "grad_norm": 2.434051752090454,
      "learning_rate": 9.19665082597873e-06,
      "loss": 0.6744,
      "step": 2880
    },
    {
      "epoch": 1.7715332926084302,
      "grad_norm": 2.733048677444458,
      "learning_rate": 9.1061326091876e-06,
      "loss": 0.6774,
      "step": 2900
    },
    {
      "epoch": 1.7837507635919365,
      "grad_norm": 2.118014097213745,
      "learning_rate": 9.01561439239647e-06,
      "loss": 0.6738,
      "step": 2920
    },
    {
      "epoch": 1.7959682345754429,
      "grad_norm": 1.6654952764511108,
      "learning_rate": 8.925096175605342e-06,
      "loss": 0.6865,
      "step": 2940
    },
    {
      "epoch": 1.8081857055589494,
      "grad_norm": 2.6237916946411133,
      "learning_rate": 8.834577958814212e-06,
      "loss": 0.6905,
      "step": 2960
    },
    {
      "epoch": 1.8204031765424558,
      "grad_norm": 1.5260601043701172,
      "learning_rate": 8.744059742023084e-06,
      "loss": 0.6871,
      "step": 2980
    },
    {
      "epoch": 1.832620647525962,
      "grad_norm": 2.2421343326568604,
      "learning_rate": 8.653541525231954e-06,
      "loss": 0.6864,
      "step": 3000
    },
    {
      "epoch": 1.8448381185094687,
      "grad_norm": 1.822084665298462,
      "learning_rate": 8.563023308440824e-06,
      "loss": 0.6854,
      "step": 3020
    },
    {
      "epoch": 1.857055589492975,
      "grad_norm": 3.7366864681243896,
      "learning_rate": 8.472505091649696e-06,
      "loss": 0.6746,
      "step": 3040
    },
    {
      "epoch": 1.8692730604764813,
      "grad_norm": 3.551156520843506,
      "learning_rate": 8.381986874858566e-06,
      "loss": 0.681,
      "step": 3060
    },
    {
      "epoch": 1.881490531459988,
      "grad_norm": 6.240765571594238,
      "learning_rate": 8.291468658067436e-06,
      "loss": 0.6856,
      "step": 3080
    },
    {
      "epoch": 1.8937080024434942,
      "grad_norm": 2.364452362060547,
      "learning_rate": 8.200950441276308e-06,
      "loss": 0.6819,
      "step": 3100
    },
    {
      "epoch": 1.9059254734270006,
      "grad_norm": 2.9040780067443848,
      "learning_rate": 8.110432224485178e-06,
      "loss": 0.6793,
      "step": 3120
    },
    {
      "epoch": 1.9181429444105071,
      "grad_norm": 2.613640785217285,
      "learning_rate": 8.01991400769405e-06,
      "loss": 0.6612,
      "step": 3140
    },
    {
      "epoch": 1.9303604153940135,
      "grad_norm": 2.9615116119384766,
      "learning_rate": 7.92939579090292e-06,
      "loss": 0.6735,
      "step": 3160
    },
    {
      "epoch": 1.9425778863775198,
      "grad_norm": 3.6287872791290283,
      "learning_rate": 7.83887757411179e-06,
      "loss": 0.6858,
      "step": 3180
    },
    {
      "epoch": 1.9547953573610264,
      "grad_norm": 2.7975776195526123,
      "learning_rate": 7.748359357320662e-06,
      "loss": 0.6595,
      "step": 3200
    },
    {
      "epoch": 1.9670128283445327,
      "grad_norm": 1.9251697063446045,
      "learning_rate": 7.657841140529532e-06,
      "loss": 0.68,
      "step": 3220
    },
    {
      "epoch": 1.979230299328039,
      "grad_norm": 2.746013879776001,
      "learning_rate": 7.5673229237384024e-06,
      "loss": 0.6975,
      "step": 3240
    },
    {
      "epoch": 1.9914477703115456,
      "grad_norm": 3.5015182495117188,
      "learning_rate": 7.4768047069472734e-06,
      "loss": 0.6739,
      "step": 3260
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.5607818961143494,
      "eval_loss": 0.6809751391410828,
      "eval_runtime": 130.248,
      "eval_samples_per_second": 50.273,
      "eval_steps_per_second": 6.288,
      "step": 3274
    },
    {
      "epoch": 2.0036652412950517,
      "grad_norm": 1.7520941495895386,
      "learning_rate": 7.386286490156145e-06,
      "loss": 0.662,
      "step": 3280
    },
    {
      "epoch": 2.0158827122785583,
      "grad_norm": 2.1755619049072266,
      "learning_rate": 7.2957682733650145e-06,
      "loss": 0.6827,
      "step": 3300
    },
    {
      "epoch": 2.028100183262065,
      "grad_norm": 2.3906071186065674,
      "learning_rate": 7.2052500565738855e-06,
      "loss": 0.6748,
      "step": 3320
    },
    {
      "epoch": 2.040317654245571,
      "grad_norm": 2.795931100845337,
      "learning_rate": 7.114731839782757e-06,
      "loss": 0.6619,
      "step": 3340
    },
    {
      "epoch": 2.0525351252290776,
      "grad_norm": 1.2882564067840576,
      "learning_rate": 7.024213622991628e-06,
      "loss": 0.6902,
      "step": 3360
    },
    {
      "epoch": 2.064752596212584,
      "grad_norm": 4.078741550445557,
      "learning_rate": 6.933695406200498e-06,
      "loss": 0.6876,
      "step": 3380
    },
    {
      "epoch": 2.0769700671960902,
      "grad_norm": 1.873988389968872,
      "learning_rate": 6.8431771894093695e-06,
      "loss": 0.6872,
      "step": 3400
    },
    {
      "epoch": 2.089187538179597,
      "grad_norm": 1.5832210779190063,
      "learning_rate": 6.7526589726182404e-06,
      "loss": 0.679,
      "step": 3420
    },
    {
      "epoch": 2.1014050091631034,
      "grad_norm": 5.315688133239746,
      "learning_rate": 6.6621407558271114e-06,
      "loss": 0.668,
      "step": 3440
    },
    {
      "epoch": 2.1136224801466095,
      "grad_norm": 7.837104320526123,
      "learning_rate": 6.571622539035981e-06,
      "loss": 0.6817,
      "step": 3460
    },
    {
      "epoch": 2.125839951130116,
      "grad_norm": 6.373603820800781,
      "learning_rate": 6.4811043222448525e-06,
      "loss": 0.6777,
      "step": 3480
    },
    {
      "epoch": 2.1380574221136226,
      "grad_norm": 3.579786539077759,
      "learning_rate": 6.3905861054537235e-06,
      "loss": 0.686,
      "step": 3500
    },
    {
      "epoch": 2.1502748930971287,
      "grad_norm": 3.1730570793151855,
      "learning_rate": 6.3000678886625945e-06,
      "loss": 0.6746,
      "step": 3520
    },
    {
      "epoch": 2.1624923640806353,
      "grad_norm": 1.4443576335906982,
      "learning_rate": 6.209549671871465e-06,
      "loss": 0.6604,
      "step": 3540
    },
    {
      "epoch": 2.174709835064142,
      "grad_norm": 3.0184175968170166,
      "learning_rate": 6.119031455080336e-06,
      "loss": 0.6724,
      "step": 3560
    },
    {
      "epoch": 2.186927306047648,
      "grad_norm": 15.886059761047363,
      "learning_rate": 6.028513238289207e-06,
      "loss": 0.6772,
      "step": 3580
    },
    {
      "epoch": 2.1991447770311545,
      "grad_norm": 1.5599966049194336,
      "learning_rate": 5.937995021498078e-06,
      "loss": 0.67,
      "step": 3600
    },
    {
      "epoch": 2.211362248014661,
      "grad_norm": 4.769693851470947,
      "learning_rate": 5.847476804706948e-06,
      "loss": 0.6878,
      "step": 3620
    },
    {
      "epoch": 2.223579718998167,
      "grad_norm": 3.966893196105957,
      "learning_rate": 5.756958587915819e-06,
      "loss": 0.6764,
      "step": 3640
    },
    {
      "epoch": 2.2357971899816738,
      "grad_norm": 7.408311367034912,
      "learning_rate": 5.66644037112469e-06,
      "loss": 0.6755,
      "step": 3660
    },
    {
      "epoch": 2.2480146609651803,
      "grad_norm": 6.423618793487549,
      "learning_rate": 5.57592215433356e-06,
      "loss": 0.6773,
      "step": 3680
    },
    {
      "epoch": 2.2602321319486864,
      "grad_norm": 1.9909167289733887,
      "learning_rate": 5.485403937542431e-06,
      "loss": 0.6646,
      "step": 3700
    },
    {
      "epoch": 2.272449602932193,
      "grad_norm": 2.2872040271759033,
      "learning_rate": 5.394885720751302e-06,
      "loss": 0.6863,
      "step": 3720
    },
    {
      "epoch": 2.2846670739156996,
      "grad_norm": 3.3011035919189453,
      "learning_rate": 5.304367503960173e-06,
      "loss": 0.6824,
      "step": 3740
    },
    {
      "epoch": 2.2968845448992057,
      "grad_norm": 2.8934574127197266,
      "learning_rate": 5.213849287169043e-06,
      "loss": 0.6792,
      "step": 3760
    },
    {
      "epoch": 2.3091020158827122,
      "grad_norm": 5.0819478034973145,
      "learning_rate": 5.123331070377914e-06,
      "loss": 0.7128,
      "step": 3780
    },
    {
      "epoch": 2.321319486866219,
      "grad_norm": 1.8844664096832275,
      "learning_rate": 5.032812853586785e-06,
      "loss": 0.668,
      "step": 3800
    },
    {
      "epoch": 2.333536957849725,
      "grad_norm": 1.4417624473571777,
      "learning_rate": 4.942294636795655e-06,
      "loss": 0.6841,
      "step": 3820
    },
    {
      "epoch": 2.3457544288332315,
      "grad_norm": 2.6393024921417236,
      "learning_rate": 4.851776420004527e-06,
      "loss": 0.6683,
      "step": 3840
    },
    {
      "epoch": 2.357971899816738,
      "grad_norm": 6.5255961418151855,
      "learning_rate": 4.761258203213397e-06,
      "loss": 0.6799,
      "step": 3860
    },
    {
      "epoch": 2.370189370800244,
      "grad_norm": 5.733590602874756,
      "learning_rate": 4.670739986422268e-06,
      "loss": 0.6687,
      "step": 3880
    },
    {
      "epoch": 2.3824068417837507,
      "grad_norm": 2.9317243099212646,
      "learning_rate": 4.580221769631138e-06,
      "loss": 0.6697,
      "step": 3900
    },
    {
      "epoch": 2.3946243127672573,
      "grad_norm": 3.0646536350250244,
      "learning_rate": 4.48970355284001e-06,
      "loss": 0.656,
      "step": 3920
    },
    {
      "epoch": 2.4068417837507634,
      "grad_norm": 1.6822599172592163,
      "learning_rate": 4.39918533604888e-06,
      "loss": 0.6728,
      "step": 3940
    },
    {
      "epoch": 2.41905925473427,
      "grad_norm": 1.9153616428375244,
      "learning_rate": 4.308667119257751e-06,
      "loss": 0.6571,
      "step": 3960
    },
    {
      "epoch": 2.4312767257177765,
      "grad_norm": 1.4584803581237793,
      "learning_rate": 4.218148902466622e-06,
      "loss": 0.6826,
      "step": 3980
    },
    {
      "epoch": 2.4434941967012827,
      "grad_norm": 5.644608020782471,
      "learning_rate": 4.127630685675493e-06,
      "loss": 0.681,
      "step": 4000
    },
    {
      "epoch": 2.455711667684789,
      "grad_norm": 1.6725857257843018,
      "learning_rate": 4.037112468884363e-06,
      "loss": 0.6631,
      "step": 4020
    },
    {
      "epoch": 2.4679291386682958,
      "grad_norm": 2.0202836990356445,
      "learning_rate": 3.946594252093234e-06,
      "loss": 0.6828,
      "step": 4040
    },
    {
      "epoch": 2.480146609651802,
      "grad_norm": 3.3473851680755615,
      "learning_rate": 3.856076035302105e-06,
      "loss": 0.687,
      "step": 4060
    },
    {
      "epoch": 2.4923640806353085,
      "grad_norm": 4.8477091789245605,
      "learning_rate": 3.765557818510976e-06,
      "loss": 0.6823,
      "step": 4080
    },
    {
      "epoch": 2.504581551618815,
      "grad_norm": 2.0793662071228027,
      "learning_rate": 3.6750396017198462e-06,
      "loss": 0.6605,
      "step": 4100
    },
    {
      "epoch": 2.5167990226023216,
      "grad_norm": 2.564861536026001,
      "learning_rate": 3.584521384928717e-06,
      "loss": 0.6746,
      "step": 4120
    },
    {
      "epoch": 2.5290164935858277,
      "grad_norm": 5.353450775146484,
      "learning_rate": 3.494003168137588e-06,
      "loss": 0.6635,
      "step": 4140
    },
    {
      "epoch": 2.5412339645693343,
      "grad_norm": 3.792398691177368,
      "learning_rate": 3.4034849513464583e-06,
      "loss": 0.685,
      "step": 4160
    },
    {
      "epoch": 2.553451435552841,
      "grad_norm": 6.40240478515625,
      "learning_rate": 3.3129667345553297e-06,
      "loss": 0.6841,
      "step": 4180
    },
    {
      "epoch": 2.565668906536347,
      "grad_norm": 3.965484142303467,
      "learning_rate": 3.2224485177642003e-06,
      "loss": 0.6866,
      "step": 4200
    },
    {
      "epoch": 2.5778863775198535,
      "grad_norm": 3.3097586631774902,
      "learning_rate": 3.1319303009730713e-06,
      "loss": 0.6822,
      "step": 4220
    },
    {
      "epoch": 2.59010384850336,
      "grad_norm": 3.1164426803588867,
      "learning_rate": 3.041412084181942e-06,
      "loss": 0.6766,
      "step": 4240
    },
    {
      "epoch": 2.602321319486866,
      "grad_norm": 5.08481502532959,
      "learning_rate": 2.950893867390813e-06,
      "loss": 0.6808,
      "step": 4260
    },
    {
      "epoch": 2.6145387904703727,
      "grad_norm": 3.7797365188598633,
      "learning_rate": 2.8603756505996834e-06,
      "loss": 0.6763,
      "step": 4280
    },
    {
      "epoch": 2.6267562614538793,
      "grad_norm": 2.4147746562957764,
      "learning_rate": 2.7698574338085544e-06,
      "loss": 0.6673,
      "step": 4300
    },
    {
      "epoch": 2.6389737324373854,
      "grad_norm": 1.5746468305587769,
      "learning_rate": 2.679339217017425e-06,
      "loss": 0.6702,
      "step": 4320
    },
    {
      "epoch": 2.651191203420892,
      "grad_norm": 1.5429770946502686,
      "learning_rate": 2.588821000226296e-06,
      "loss": 0.6782,
      "step": 4340
    },
    {
      "epoch": 2.6634086744043985,
      "grad_norm": 2.6828017234802246,
      "learning_rate": 2.4983027834351665e-06,
      "loss": 0.6823,
      "step": 4360
    },
    {
      "epoch": 2.6756261453879047,
      "grad_norm": 2.3820037841796875,
      "learning_rate": 2.407784566644037e-06,
      "loss": 0.6556,
      "step": 4380
    },
    {
      "epoch": 2.6878436163714112,
      "grad_norm": 1.54110848903656,
      "learning_rate": 2.317266349852908e-06,
      "loss": 0.6832,
      "step": 4400
    },
    {
      "epoch": 2.700061087354918,
      "grad_norm": 2.0336530208587646,
      "learning_rate": 2.226748133061779e-06,
      "loss": 0.6827,
      "step": 4420
    },
    {
      "epoch": 2.712278558338424,
      "grad_norm": 2.504866123199463,
      "learning_rate": 2.1362299162706496e-06,
      "loss": 0.6876,
      "step": 4440
    },
    {
      "epoch": 2.7244960293219305,
      "grad_norm": 2.4451675415039062,
      "learning_rate": 2.0457116994795205e-06,
      "loss": 0.6927,
      "step": 4460
    },
    {
      "epoch": 2.736713500305437,
      "grad_norm": 2.7124907970428467,
      "learning_rate": 1.955193482688391e-06,
      "loss": 0.659,
      "step": 4480
    },
    {
      "epoch": 2.748930971288943,
      "grad_norm": 3.7362606525421143,
      "learning_rate": 1.8646752658972619e-06,
      "loss": 0.6709,
      "step": 4500
    },
    {
      "epoch": 2.7611484422724497,
      "grad_norm": 16.127742767333984,
      "learning_rate": 1.7741570491061326e-06,
      "loss": 0.6742,
      "step": 4520
    },
    {
      "epoch": 2.7733659132559563,
      "grad_norm": 3.389012098312378,
      "learning_rate": 1.6836388323150036e-06,
      "loss": 0.684,
      "step": 4540
    },
    {
      "epoch": 2.7855833842394624,
      "grad_norm": 5.569704532623291,
      "learning_rate": 1.5931206155238744e-06,
      "loss": 0.6693,
      "step": 4560
    },
    {
      "epoch": 2.797800855222969,
      "grad_norm": 1.6636743545532227,
      "learning_rate": 1.5026023987327452e-06,
      "loss": 0.6784,
      "step": 4580
    },
    {
      "epoch": 2.8100183262064755,
      "grad_norm": 2.9547157287597656,
      "learning_rate": 1.412084181941616e-06,
      "loss": 0.6849,
      "step": 4600
    },
    {
      "epoch": 2.8222357971899816,
      "grad_norm": 1.924782633781433,
      "learning_rate": 1.3215659651504867e-06,
      "loss": 0.6765,
      "step": 4620
    },
    {
      "epoch": 2.834453268173488,
      "grad_norm": 1.9082609415054321,
      "learning_rate": 1.2310477483593575e-06,
      "loss": 0.6884,
      "step": 4640
    },
    {
      "epoch": 2.8466707391569948,
      "grad_norm": 2.4597978591918945,
      "learning_rate": 1.1405295315682282e-06,
      "loss": 0.671,
      "step": 4660
    },
    {
      "epoch": 2.858888210140501,
      "grad_norm": 2.3618741035461426,
      "learning_rate": 1.050011314777099e-06,
      "loss": 0.6725,
      "step": 4680
    },
    {
      "epoch": 2.8711056811240074,
      "grad_norm": 2.669123888015747,
      "learning_rate": 9.594930979859698e-07,
      "loss": 0.6647,
      "step": 4700
    },
    {
      "epoch": 2.883323152107514,
      "grad_norm": 6.436647891998291,
      "learning_rate": 8.689748811948405e-07,
      "loss": 0.6716,
      "step": 4720
    },
    {
      "epoch": 2.89554062309102,
      "grad_norm": 3.7726078033447266,
      "learning_rate": 7.784566644037113e-07,
      "loss": 0.6753,
      "step": 4740
    },
    {
      "epoch": 2.9077580940745267,
      "grad_norm": 2.1796865463256836,
      "learning_rate": 6.879384476125821e-07,
      "loss": 0.6789,
      "step": 4760
    },
    {
      "epoch": 2.9199755650580332,
      "grad_norm": 2.186727523803711,
      "learning_rate": 5.974202308214529e-07,
      "loss": 0.6796,
      "step": 4780
    },
    {
      "epoch": 2.9321930360415394,
      "grad_norm": 1.8810961246490479,
      "learning_rate": 5.069020140303236e-07,
      "loss": 0.6711,
      "step": 4800
    },
    {
      "epoch": 2.944410507025046,
      "grad_norm": 3.104782819747925,
      "learning_rate": 4.1638379723919446e-07,
      "loss": 0.6596,
      "step": 4820
    },
    {
      "epoch": 2.9566279780085525,
      "grad_norm": 1.945496916770935,
      "learning_rate": 3.258655804480652e-07,
      "loss": 0.6684,
      "step": 4840
    },
    {
      "epoch": 2.9688454489920586,
      "grad_norm": 2.737485408782959,
      "learning_rate": 2.35347363656936e-07,
      "loss": 0.6685,
      "step": 4860
    },
    {
      "epoch": 2.981062919975565,
      "grad_norm": 2.983844518661499,
      "learning_rate": 1.4482914686580675e-07,
      "loss": 0.6583,
      "step": 4880
    },
    {
      "epoch": 2.9932803909590717,
      "grad_norm": 1.74317467212677,
      "learning_rate": 5.431093007467753e-08,
      "loss": 0.663,
      "step": 4900
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.5653634667396545,
      "eval_loss": 0.6802251935005188,
      "eval_runtime": 112.6587,
      "eval_samples_per_second": 58.122,
      "eval_steps_per_second": 7.27,
      "step": 4911
    }
  ],
  "logging_steps": 20,
  "max_steps": 4911,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.718152893476864e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
